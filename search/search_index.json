{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"JEO-Batch tutorial (for first time users!) \u00b6 This tutorial focuses on users who want to get started with JEO-Batch without entering into too many details. Prerequisites \u00b6 In order to use JEO-Batch you must have: an account on BDA access to the JEO-Desk basic linux terminal knowledge Assuming you have these, you can proceed to the User Guide.","title":"Home"},{"location":"#jeo-batch-tutorial-for-first-time-users","text":"This tutorial focuses on users who want to get started with JEO-Batch without entering into too many details.","title":"JEO-Batch tutorial (for first time users!)"},{"location":"#prerequisites","text":"In order to use JEO-Batch you must have: an account on BDA access to the JEO-Desk basic linux terminal knowledge Assuming you have these, you can proceed to the User Guide.","title":"Prerequisites"},{"location":"00_set_things_up/","text":"Setting things up \u00b6 In order to be allowed to use JEO-Batch and write to EOS, your user must belong to one (or more) groups. You can find the groups to which your user belongs to with the command id : $ id mavropa uid = 35861 ( mavropa ) gid = 40507 ( EC_JRC_BigDataEOSS_CORE ) groups = 40507 ( EC_JRC_BigDataEOSS_CORE ) ,504 ( cid_rdp ) ,507 ( JEODPP_notebook ) ,7921 ( TOL_CONTEU ) ,29431 ( cid_dvpt ) ,40600 ( EU_EC ) ,41068 ( EC_JRC ) ,50007 ( EC_JRC_P_GHSL ) In JEO-Batch context, the groups which are of interest are those that start with EC_JRC_P_* . As you can see the user mavropa , currently, belongs to a bunch of groups among which there is EC_JRC_P_GHSL . This means that mavropa : has read/write permissions on GHSL's project directory on EOS (i.e. /eos/jeodpp/data/projects/GHSL ). is allowed to use JEO-Batch via the ghslproc user. Note For each JRC project there is a corresponding EC_JRC_P_* group and a corresponding *proc user. For example, for the CRITECH project, there is the corresponding EC_JRC_P_CRITECH group and the critechproc user. Tip If mavropa also belonged to e.g. EC_JRC_P_INCA , then he would have also been able to use JEO-Batch via the incaproc user. Warning It is ghslproc that we will be using throughout this tutorial, but chances are that you will not belong in that group. So, In order to continue, please check your id , identify your EC_JRC_P_* group and in the following commands, replace ghslproc user with the proc user of your own group! To test that everything works properly please run the following command: sudo -u ghslproc condor_q sudo has been configured to not require a password when used with condor_* commands. So, you shouldn't get a password prompt. The result should be something like this: $ sudo -u ghslproc condor_q -- Schedd: s-jrciprjeop161p.cidsn.jrc.it : < 139 .191.240.161:9618?... @ 07 /06/20 12 :49:31 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE HOLD TOTAL JOB_IDS Total for ghslproc: 0 jobs ; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Total for all users: 410 jobs ; 0 completed, 0 removed, 86 idle, 15 running, 309 held, 0 suspended If this works then everything is fine and you can try to submit your first job. If not, then there is some problem and you are advised to open a ticket","title":"Set things up"},{"location":"00_set_things_up/#setting-things-up","text":"In order to be allowed to use JEO-Batch and write to EOS, your user must belong to one (or more) groups. You can find the groups to which your user belongs to with the command id : $ id mavropa uid = 35861 ( mavropa ) gid = 40507 ( EC_JRC_BigDataEOSS_CORE ) groups = 40507 ( EC_JRC_BigDataEOSS_CORE ) ,504 ( cid_rdp ) ,507 ( JEODPP_notebook ) ,7921 ( TOL_CONTEU ) ,29431 ( cid_dvpt ) ,40600 ( EU_EC ) ,41068 ( EC_JRC ) ,50007 ( EC_JRC_P_GHSL ) In JEO-Batch context, the groups which are of interest are those that start with EC_JRC_P_* . As you can see the user mavropa , currently, belongs to a bunch of groups among which there is EC_JRC_P_GHSL . This means that mavropa : has read/write permissions on GHSL's project directory on EOS (i.e. /eos/jeodpp/data/projects/GHSL ). is allowed to use JEO-Batch via the ghslproc user. Note For each JRC project there is a corresponding EC_JRC_P_* group and a corresponding *proc user. For example, for the CRITECH project, there is the corresponding EC_JRC_P_CRITECH group and the critechproc user. Tip If mavropa also belonged to e.g. EC_JRC_P_INCA , then he would have also been able to use JEO-Batch via the incaproc user. Warning It is ghslproc that we will be using throughout this tutorial, but chances are that you will not belong in that group. So, In order to continue, please check your id , identify your EC_JRC_P_* group and in the following commands, replace ghslproc user with the proc user of your own group! To test that everything works properly please run the following command: sudo -u ghslproc condor_q sudo has been configured to not require a password when used with condor_* commands. So, you shouldn't get a password prompt. The result should be something like this: $ sudo -u ghslproc condor_q -- Schedd: s-jrciprjeop161p.cidsn.jrc.it : < 139 .191.240.161:9618?... @ 07 /06/20 12 :49:31 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE HOLD TOTAL JOB_IDS Total for ghslproc: 0 jobs ; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Total for all users: 410 jobs ; 0 completed, 0 removed, 86 idle, 15 running, 309 held, 0 suspended If this works then everything is fine and you can try to submit your first job. If not, then there is some problem and you are advised to open a ticket","title":"Setting things up"},{"location":"01_the_first_job/","text":"Submitting our first job \u00b6 Where to work from? \u00b6 The first thing we need to do is to change the Current Working Directory (CWD) to a directory where both our regular user and the ghslproc user have write access. A good example of such a directory is the GHSL project home directory on EOS. This is located at: cd /eos/jeodpp/data/projects/GHSL Warning Obviously you need to change GHSL with you own project name. Since the project home directory is shared among multiple people, it is usually a good idea to create a subdirectory with your user name and operate in there. So please run this: cd /eos/jeodpp/data/projects/GHSL mkdir $( whoami ) cd $( whoami ) Submission file \u00b6 In order to submit a job we must first create a text file with describes the job. For now please create a file named job_description with the following contents: universe = docker docker_image = jeoreg.cidsn.jrc.it:5000/jeodpp-htcondor/jeodpp_base_gdal_py3_deb10:1.0 executable = /bin/cat arguments = /etc/hosts output = hello.$(ClusterId).$(ProcId).out error = hello.$(ClusterId).$(ProcId).err log = hello.$(ClusterId).$(ProcId).log request_memory = 100M queue 1 Note The actual filename is not really important, but job_description is what we will be using throughout this tutorial. We will not go through all the details now. In a nutshell, this is the description of a job, which, when submitted, will create a docker container in one of the JEO-Batch nodes and will execute /bin/cat /etc/hosts . Submit the job \u00b6 Now, please proceed to submit the job with: sudo -u ghslproc condor_submit job_description . You should see something like this: $ sudo -u ghslproc condor_submit job_description Submitting job ( s ) . 1 job ( s ) submitted to cluster 40152 . Take notice of the number 40152 (yours will obviously be different). This is the JOB_ID . We will need it later on. You can check the status of the job with: condor_q . You should see something like this: $ sudo -u ghslproc condor_q -- Schedd: s-jrciprjeop161p.cidsn.jrc.it : < 139 .191.240.161:9618?... @ 07 /06/20 16 :18:53 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS ghslproc ID: 40152 7 /6 16 :18 _ 1 _ 1 40152 .0 Total for query: 1 jobs ; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for ghslproc: 1 jobs ; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 411 jobs ; 0 completed, 0 removed, 86 idle, 16 running, 309 held, 0 suspended Tip The job we submited is trivial. It should be finished in just a few seconds. If you don't see any jobs when you run condor_q then chances are that the job finished sucecessfully. Sumbit another job and try to be a little faster this time) Note Do notice the \"owner\". It is ghslproc . In your case it will be your own *proc user! Do notice the RUN column. The 1 underneath means that the container has already been deployed and that the job is currently running. If the JEO-Batch cluster is very busy, you might notice that your job is under IDLE instead. If that is the case just wait for a while and check the status again. If it remains indefinitely on IDLE, then chances are that you requested too many resources (e.g. too much RAM). If that is not the case, please open a ticket . Do also notice the JOB_IDS column. As you can see it is the same JOB_ID we got when we submitted the job. After a few seconds the job should have finished. Whet it is finished there should be no jobs for ghslproc : $ sudo -u ghslproc condor_q -- Schedd: s-jrciprjeop161p.cidsn.jrc.it : < 139 .191.240.161:9618?... @ 07 /06/20 16 :22:24 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE HOLD TOTAL JOB_IDS Total for query: 0 jobs ; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Total for ghslproc: 0 jobs ; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Total for all users: 410 jobs ; 0 completed, 0 removed, 87 idle, 14 running, 309 held, 0 suspended Check output \u00b6 Now that the job has been finished, there should be 3 new files in our working directory: hello.40152.0.out which contains the STDOUT of the container hello.40152.0.err which contains the STDERR of the container hello.40152.0.log which contains some logs which come from HTCondor Tip Obviously, the JOB_ID of your files will be different. Please adjust as necessary. $ ls -lah hello* -rw-r--r-- 1 ghslproc EC_JRC_P_GHSL 0 Jul 9 10 :42 hello.40152.0.err -rw-r--r-- 1 ghslproc EC_JRC_P_GHSL 942 Jul 9 10 :42 hello.40152.0.log -rw-r--r-- 1 ghslproc EC_JRC_P_GHSL 223 Jul 9 10 :42 hello.40152.0.out As you can see, in our example, the STDERR file is empty while the STDOUT error contains the contents of /etc/hosts of the container that got created. We can easily check this with: $ cat hello.40152.0.out 127 .0.0.1localhost ::1localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 172 .17.0.2ghslproc-40152.0-s-jrciprjeop194p.cidsn.jrc.it ghslproc-40152 Congratulations! You just managed to use JEO-Batch successfully! Cleanup \u00b6 Just remove the log files with: rm -f hello*","title":"The first job"},{"location":"01_the_first_job/#submitting-our-first-job","text":"","title":"Submitting our first job"},{"location":"01_the_first_job/#where-to-work-from","text":"The first thing we need to do is to change the Current Working Directory (CWD) to a directory where both our regular user and the ghslproc user have write access. A good example of such a directory is the GHSL project home directory on EOS. This is located at: cd /eos/jeodpp/data/projects/GHSL Warning Obviously you need to change GHSL with you own project name. Since the project home directory is shared among multiple people, it is usually a good idea to create a subdirectory with your user name and operate in there. So please run this: cd /eos/jeodpp/data/projects/GHSL mkdir $( whoami ) cd $( whoami )","title":"Where to work from?"},{"location":"01_the_first_job/#submission-file","text":"In order to submit a job we must first create a text file with describes the job. For now please create a file named job_description with the following contents: universe = docker docker_image = jeoreg.cidsn.jrc.it:5000/jeodpp-htcondor/jeodpp_base_gdal_py3_deb10:1.0 executable = /bin/cat arguments = /etc/hosts output = hello.$(ClusterId).$(ProcId).out error = hello.$(ClusterId).$(ProcId).err log = hello.$(ClusterId).$(ProcId).log request_memory = 100M queue 1 Note The actual filename is not really important, but job_description is what we will be using throughout this tutorial. We will not go through all the details now. In a nutshell, this is the description of a job, which, when submitted, will create a docker container in one of the JEO-Batch nodes and will execute /bin/cat /etc/hosts .","title":"Submission file"},{"location":"01_the_first_job/#submit-the-job","text":"Now, please proceed to submit the job with: sudo -u ghslproc condor_submit job_description . You should see something like this: $ sudo -u ghslproc condor_submit job_description Submitting job ( s ) . 1 job ( s ) submitted to cluster 40152 . Take notice of the number 40152 (yours will obviously be different). This is the JOB_ID . We will need it later on. You can check the status of the job with: condor_q . You should see something like this: $ sudo -u ghslproc condor_q -- Schedd: s-jrciprjeop161p.cidsn.jrc.it : < 139 .191.240.161:9618?... @ 07 /06/20 16 :18:53 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS ghslproc ID: 40152 7 /6 16 :18 _ 1 _ 1 40152 .0 Total for query: 1 jobs ; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for ghslproc: 1 jobs ; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 411 jobs ; 0 completed, 0 removed, 86 idle, 16 running, 309 held, 0 suspended Tip The job we submited is trivial. It should be finished in just a few seconds. If you don't see any jobs when you run condor_q then chances are that the job finished sucecessfully. Sumbit another job and try to be a little faster this time) Note Do notice the \"owner\". It is ghslproc . In your case it will be your own *proc user! Do notice the RUN column. The 1 underneath means that the container has already been deployed and that the job is currently running. If the JEO-Batch cluster is very busy, you might notice that your job is under IDLE instead. If that is the case just wait for a while and check the status again. If it remains indefinitely on IDLE, then chances are that you requested too many resources (e.g. too much RAM). If that is not the case, please open a ticket . Do also notice the JOB_IDS column. As you can see it is the same JOB_ID we got when we submitted the job. After a few seconds the job should have finished. Whet it is finished there should be no jobs for ghslproc : $ sudo -u ghslproc condor_q -- Schedd: s-jrciprjeop161p.cidsn.jrc.it : < 139 .191.240.161:9618?... @ 07 /06/20 16 :22:24 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE HOLD TOTAL JOB_IDS Total for query: 0 jobs ; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Total for ghslproc: 0 jobs ; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Total for all users: 410 jobs ; 0 completed, 0 removed, 87 idle, 14 running, 309 held, 0 suspended","title":"Submit the job"},{"location":"01_the_first_job/#check-output","text":"Now that the job has been finished, there should be 3 new files in our working directory: hello.40152.0.out which contains the STDOUT of the container hello.40152.0.err which contains the STDERR of the container hello.40152.0.log which contains some logs which come from HTCondor Tip Obviously, the JOB_ID of your files will be different. Please adjust as necessary. $ ls -lah hello* -rw-r--r-- 1 ghslproc EC_JRC_P_GHSL 0 Jul 9 10 :42 hello.40152.0.err -rw-r--r-- 1 ghslproc EC_JRC_P_GHSL 942 Jul 9 10 :42 hello.40152.0.log -rw-r--r-- 1 ghslproc EC_JRC_P_GHSL 223 Jul 9 10 :42 hello.40152.0.out As you can see, in our example, the STDERR file is empty while the STDOUT error contains the contents of /etc/hosts of the container that got created. We can easily check this with: $ cat hello.40152.0.out 127 .0.0.1localhost ::1localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 172 .17.0.2ghslproc-40152.0-s-jrciprjeop194p.cidsn.jrc.it ghslproc-40152 Congratulations! You just managed to use JEO-Batch successfully!","title":"Check output"},{"location":"01_the_first_job/#cleanup","text":"Just remove the log files with: rm -f hello*","title":"Cleanup"},{"location":"02_when_things_go_wrong/","text":"When things go wrong \u00b6 Now let's try to do something more interesting. Let's submit a job that fails. A failing job \u00b6 Please edit the job_description file and change the arguments line from /etc/hosts to /etc/does_not_exist : -arguments = /etc/hosts +arguments = /etc/does/not/exist In other words we are asking condor to spawn a container that will execute: cat /etc/does/not/exist To make things easier, the full job_description file is here: universe = docker docker_image = jeoreg.cidsn.jrc.it:5000/jeodpp-htcondor/jeodpp_base_gdal_py3_deb10:1.0 executable = /bin/cat arguments = /etc/does/not/exist output = hello. $( ClusterId ) . $( ProcId ) .out error = hello. $( ClusterId ) . $( ProcId ) .err log = hello. $( ClusterId ) . $( ProcId ) .log request_memory = 100M queue 1 Submit the file: sudo - u ghslproc condor_submit job_description Submitting job ( s ). 1 job ( s ) submitted to cluster 40164 . Wait until the job finishes and check the files containing STDOUT and STDERR output: $ ls -lah hello* -rw-r--r-- 1 ghslproc EC_JRC_P_GHSL 57 Jul 9 11 :39 hello.40164.0.err -rw-r--r-- 1 ghslproc EC_JRC_P_GHSL 942 Jul 9 11 :39 hello.40164.0.log -rw-r--r-- 1 ghslproc EC_JRC_P_GHSL 0 Jul 9 11 :39 hello.40164.0.out As we can see, the STDOUT file is empty, while the STDERR contains: $ cat hello.40164.0.err /bin/cat: /etc/does/not/exist: No such file or directory Congratulations! You now know how to to see what went wrong with your scripts! Cleanup \u00b6 Just remove the log files with: rm -f hello*","title":"When things go wrong"},{"location":"02_when_things_go_wrong/#when-things-go-wrong","text":"Now let's try to do something more interesting. Let's submit a job that fails.","title":"When things go wrong"},{"location":"02_when_things_go_wrong/#a-failing-job","text":"Please edit the job_description file and change the arguments line from /etc/hosts to /etc/does_not_exist : -arguments = /etc/hosts +arguments = /etc/does/not/exist In other words we are asking condor to spawn a container that will execute: cat /etc/does/not/exist To make things easier, the full job_description file is here: universe = docker docker_image = jeoreg.cidsn.jrc.it:5000/jeodpp-htcondor/jeodpp_base_gdal_py3_deb10:1.0 executable = /bin/cat arguments = /etc/does/not/exist output = hello. $( ClusterId ) . $( ProcId ) .out error = hello. $( ClusterId ) . $( ProcId ) .err log = hello. $( ClusterId ) . $( ProcId ) .log request_memory = 100M queue 1 Submit the file: sudo - u ghslproc condor_submit job_description Submitting job ( s ). 1 job ( s ) submitted to cluster 40164 . Wait until the job finishes and check the files containing STDOUT and STDERR output: $ ls -lah hello* -rw-r--r-- 1 ghslproc EC_JRC_P_GHSL 57 Jul 9 11 :39 hello.40164.0.err -rw-r--r-- 1 ghslproc EC_JRC_P_GHSL 942 Jul 9 11 :39 hello.40164.0.log -rw-r--r-- 1 ghslproc EC_JRC_P_GHSL 0 Jul 9 11 :39 hello.40164.0.out As we can see, the STDOUT file is empty, while the STDERR contains: $ cat hello.40164.0.err /bin/cat: /etc/does/not/exist: No such file or directory Congratulations! You now know how to to see what went wrong with your scripts!","title":"A failing job"},{"location":"02_when_things_go_wrong/#cleanup","text":"Just remove the log files with: rm -f hello*","title":"Cleanup"},{"location":"03_hold_your_horses/","text":"Hold your horses jobs \u00b6 Sometimes, after submitting a job, condor_q will show that the job is under the HOLD column. This implies that there is some sort of a problem. The problem can be in the job_description file, in the docker image that you chose, in the script that you executed etc. Tip A really common case is that too many resources (e.g. RAM) was requested A job that ends up in HOLD \u00b6 We can easily simulate such a problem be specifying an executable that does not exist. So please apply the following patch to the job_description file: -executable = /bin/cat +executable = /bin/does_not_exist For your convenience, the full job_description file is here: universe = docker docker_image = jeoreg.cidsn.jrc.it:5000/jeodpp-htcondor/jeodpp_base_gdal_py3_deb10:1.0 executable = /bin/does_not_exist arguments = /etc/hosts output = hello. $( ClusterId ) . $( ProcId ) .out error = hello. $( ClusterId ) . $( ProcId ) .err log = hello. $( ClusterId ) . $( ProcId ) .log request_memory = 100M queue 1 As usual, submit the file and check the status with condor_q : sudo -u ghslproc condor_submit job_description sudo -u ghslproc condor_q You should see something like this: -- Schedd: s-jrciprjeop161p.cidsn.jrc.it : < 139 .191.240.161:9618?... @ 07 /06/20 17 :01:53 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE HOLD TOTAL JOB_IDS ghslproc ID: 40164 7 /6 17 :01 _ _ _ 1 1 40164 .0 Total for query: 1 jobs ; 0 completed, 0 removed, 0 idle, 0 running, 1 held, 0 suspended Total for ghslproc: 1 jobs ; 0 completed, 0 removed, 0 idle, 0 running, 1 held, 0 suspended Total for all users: 412 jobs ; 0 completed, 0 removed, 87 idle, 15 running, 310 held, 0 suspended Do notice that this time the job is on HOLD . And it will remain there until we manually remove it. To further debug what is going on we need to run condor_q with the -hold argument: sudo -u ghslproc condor_q -hold The output should be the following: -- Schedd: s-jrciprjeop161p.cidsn.jrc.it : <139.191.240.161:9618?... @ 07/06/20 17:01:58 ID OWNER HELD_SINCE HOLD_REASON 40164.0 ghslproc 7 / 6 17 : 01 Error from slot1_1 @s - jrciprjeop194p . cidsn . jrc . it : Error running docker job : OCI runtime create failed : container_linux . go : 344 : starting container process caused 'exec: \\' / bin / does_not_exist \\ ': stat /bin/does_not_exist: no such file or directory' : unknown Total for query : 1 jobs ; 0 completed , 0 removed , 0 idle , 0 running , 1 held , 0 suspended Total for ghslproc : 1 jobs ; 0 completed , 0 removed , 0 idle , 0 running , 1 held , 0 suspended Total for all users : 412 jobs ; 0 completed , 0 removed , 87 idle , 15 running , 310 held , 0 suspended The HOLD_REASON column describes what exactly is the problem (in this case that the executable does not exist). Tip In case the error message is cryptic and we can't figure out how to fix it, we should open a ticket Remove the held job \u00b6 Since we can't do much more with this particular job, we should remove it with the condor_rm command and the JOB_ID: sudo -u ghslproc condor_rm 40164.0 Cleanup \u00b6 Just remove the log files with: rm -f hello*","title":"Hold your jobs!"},{"location":"03_hold_your_horses/#hold-your-horses-jobs","text":"Sometimes, after submitting a job, condor_q will show that the job is under the HOLD column. This implies that there is some sort of a problem. The problem can be in the job_description file, in the docker image that you chose, in the script that you executed etc. Tip A really common case is that too many resources (e.g. RAM) was requested","title":"Hold your horses jobs"},{"location":"03_hold_your_horses/#a-job-that-ends-up-in-hold","text":"We can easily simulate such a problem be specifying an executable that does not exist. So please apply the following patch to the job_description file: -executable = /bin/cat +executable = /bin/does_not_exist For your convenience, the full job_description file is here: universe = docker docker_image = jeoreg.cidsn.jrc.it:5000/jeodpp-htcondor/jeodpp_base_gdal_py3_deb10:1.0 executable = /bin/does_not_exist arguments = /etc/hosts output = hello. $( ClusterId ) . $( ProcId ) .out error = hello. $( ClusterId ) . $( ProcId ) .err log = hello. $( ClusterId ) . $( ProcId ) .log request_memory = 100M queue 1 As usual, submit the file and check the status with condor_q : sudo -u ghslproc condor_submit job_description sudo -u ghslproc condor_q You should see something like this: -- Schedd: s-jrciprjeop161p.cidsn.jrc.it : < 139 .191.240.161:9618?... @ 07 /06/20 17 :01:53 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE HOLD TOTAL JOB_IDS ghslproc ID: 40164 7 /6 17 :01 _ _ _ 1 1 40164 .0 Total for query: 1 jobs ; 0 completed, 0 removed, 0 idle, 0 running, 1 held, 0 suspended Total for ghslproc: 1 jobs ; 0 completed, 0 removed, 0 idle, 0 running, 1 held, 0 suspended Total for all users: 412 jobs ; 0 completed, 0 removed, 87 idle, 15 running, 310 held, 0 suspended Do notice that this time the job is on HOLD . And it will remain there until we manually remove it. To further debug what is going on we need to run condor_q with the -hold argument: sudo -u ghslproc condor_q -hold The output should be the following: -- Schedd: s-jrciprjeop161p.cidsn.jrc.it : <139.191.240.161:9618?... @ 07/06/20 17:01:58 ID OWNER HELD_SINCE HOLD_REASON 40164.0 ghslproc 7 / 6 17 : 01 Error from slot1_1 @s - jrciprjeop194p . cidsn . jrc . it : Error running docker job : OCI runtime create failed : container_linux . go : 344 : starting container process caused 'exec: \\' / bin / does_not_exist \\ ': stat /bin/does_not_exist: no such file or directory' : unknown Total for query : 1 jobs ; 0 completed , 0 removed , 0 idle , 0 running , 1 held , 0 suspended Total for ghslproc : 1 jobs ; 0 completed , 0 removed , 0 idle , 0 running , 1 held , 0 suspended Total for all users : 412 jobs ; 0 completed , 0 removed , 87 idle , 15 running , 310 held , 0 suspended The HOLD_REASON column describes what exactly is the problem (in this case that the executable does not exist). Tip In case the error message is cryptic and we can't figure out how to fix it, we should open a ticket","title":"A job that ends up in HOLD"},{"location":"03_hold_your_horses/#remove-the-held-job","text":"Since we can't do much more with this particular job, we should remove it with the condor_rm command and the JOB_ID: sudo -u ghslproc condor_rm 40164.0","title":"Remove the held job"},{"location":"03_hold_your_horses/#cleanup","text":"Just remove the log files with: rm -f hello*","title":"Cleanup"},{"location":"04_copy_files_over/","text":"Copy files over \u00b6 All the examples we've seen so far were pretty much trivial. In order to be able to do more interesting stuff, we usually need to COPY one more files to the container in order to customize its behaviour. These files can be either a script that will get executed or files that will be passed as arguments to the executable . Note Strictly speaking, copying files over is not necessary. Instead of that we can choose a docker image which already contains the script we want to execute. Nevertheless, this requires that a specific docker image has been prepared for us. Create an executable script \u00b6 Let's start by creating a custom executable shell script. Open a new file named my_script.sh and place the following contents: 1 2 3 4 5 6 7 8 9 10 #!/usr/bin/env bash # # http://redsymbol.net/articles/unofficial-bash-strict-mode/ set -euo pipefail echo \"The current host is: $( hostname ) echo \" The current user is: $( id -u ) \" echo \" The present working directory is: $( pwd ) \" echo \" The arguments passed to the script are: ${ @ } \" Now, make it executable: chmod +x ./my_script.sh And test it out. E.g.: $ ./my_script.sh 123 'Michael Jordan' 'The answer is 42' The current host is: jeodpp-text-terminal-144p-03 The current user is: 35861 The present working directory is: /eos/jeodpp/data/projects/GHSL/mavropa The arguments passed to the script are: 123 Michael Jordan The answer is 42 Copy files to the container \u00b6 In order to pass the file to the container, we need to add a couple of entries in our job_description file. transfer_input_files = my_script.sh should_transfer_files = YES when_to_transfer_output = ON_EXIT Tip If you want to copy over multiple files, you can set transfer_input_files to a comma separated string. E.g. file1,file2,file3 Furthermore, we should also change the executable entry to my_script.sh , too. For your convenience the full job_description file is this: universe = docker docker_image = jeoreg.cidsn.jrc.it:5000/jeodpp-htcondor/jeodpp_base_gdal_py3_deb10:1.0 transfer_input_files = my_script.sh should_transfer_files = YES when_to_transfer_output = ON_EXIT executable = my_script.sh arguments = 123 'Michael Jordan' 'The answer is 42' output = hello. $( ClusterId ) . $( ProcId ) .out error = hello. $( ClusterId ) . $( ProcId ) .err log = hello. $( ClusterId ) . $( ProcId ) .log request_memory = 100M queue 1 Submit the job and check the output file: $ sudo -u ghslproc condor_submit job_description Submitting job ( s ) . 1 job ( s ) submitted to cluster 40297 . # wait a few seconds... $ cat hello.40297.0.out The current host is: ghslproc-40297.0-s-jrciprjeop194p.cidsn.jrc.it The current user 's UID is: 35637 The present working directory is: /var/lib/condor/execute/dir_15155 The arguments passed to the script are: 123 ' Michael Jordan ' ' The answer is 42 ' And that was it! We executed a custom script via JEO-Batch! What should I use for executable \u00b6 A relatively subtle point is the executable entry in the job_description file. What you should keep in mind is that: if you use an absolute path (e.g. /bin/cat then the executable file must be present in the docker image. if you use a relative path (e.g. my_script.sh ) then the executable file must be copied over to the container (i.e. you must add it to transfer_input_files , too). Cleanup \u00b6 Just remove the log files with: rm -f hello*","title":"Copy files over"},{"location":"04_copy_files_over/#copy-files-over","text":"All the examples we've seen so far were pretty much trivial. In order to be able to do more interesting stuff, we usually need to COPY one more files to the container in order to customize its behaviour. These files can be either a script that will get executed or files that will be passed as arguments to the executable . Note Strictly speaking, copying files over is not necessary. Instead of that we can choose a docker image which already contains the script we want to execute. Nevertheless, this requires that a specific docker image has been prepared for us.","title":"Copy files over"},{"location":"04_copy_files_over/#create-an-executable-script","text":"Let's start by creating a custom executable shell script. Open a new file named my_script.sh and place the following contents: 1 2 3 4 5 6 7 8 9 10 #!/usr/bin/env bash # # http://redsymbol.net/articles/unofficial-bash-strict-mode/ set -euo pipefail echo \"The current host is: $( hostname ) echo \" The current user is: $( id -u ) \" echo \" The present working directory is: $( pwd ) \" echo \" The arguments passed to the script are: ${ @ } \" Now, make it executable: chmod +x ./my_script.sh And test it out. E.g.: $ ./my_script.sh 123 'Michael Jordan' 'The answer is 42' The current host is: jeodpp-text-terminal-144p-03 The current user is: 35861 The present working directory is: /eos/jeodpp/data/projects/GHSL/mavropa The arguments passed to the script are: 123 Michael Jordan The answer is 42","title":"Create an executable script"},{"location":"04_copy_files_over/#copy-files-to-the-container","text":"In order to pass the file to the container, we need to add a couple of entries in our job_description file. transfer_input_files = my_script.sh should_transfer_files = YES when_to_transfer_output = ON_EXIT Tip If you want to copy over multiple files, you can set transfer_input_files to a comma separated string. E.g. file1,file2,file3 Furthermore, we should also change the executable entry to my_script.sh , too. For your convenience the full job_description file is this: universe = docker docker_image = jeoreg.cidsn.jrc.it:5000/jeodpp-htcondor/jeodpp_base_gdal_py3_deb10:1.0 transfer_input_files = my_script.sh should_transfer_files = YES when_to_transfer_output = ON_EXIT executable = my_script.sh arguments = 123 'Michael Jordan' 'The answer is 42' output = hello. $( ClusterId ) . $( ProcId ) .out error = hello. $( ClusterId ) . $( ProcId ) .err log = hello. $( ClusterId ) . $( ProcId ) .log request_memory = 100M queue 1 Submit the job and check the output file: $ sudo -u ghslproc condor_submit job_description Submitting job ( s ) . 1 job ( s ) submitted to cluster 40297 . # wait a few seconds... $ cat hello.40297.0.out The current host is: ghslproc-40297.0-s-jrciprjeop194p.cidsn.jrc.it The current user 's UID is: 35637 The present working directory is: /var/lib/condor/execute/dir_15155 The arguments passed to the script are: 123 ' Michael Jordan ' ' The answer is 42 ' And that was it! We executed a custom script via JEO-Batch!","title":"Copy files to the container"},{"location":"04_copy_files_over/#what-should-i-use-for-executable","text":"A relatively subtle point is the executable entry in the job_description file. What you should keep in mind is that: if you use an absolute path (e.g. /bin/cat then the executable file must be present in the docker image. if you use a relative path (e.g. my_script.sh ) then the executable file must be copied over to the container (i.e. you must add it to transfer_input_files , too).","title":"What should I use for executable"},{"location":"04_copy_files_over/#cleanup","text":"Just remove the log files with: rm -f hello*","title":"Cleanup"}]}